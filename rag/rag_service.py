# rag_service_text2sql.py

import os
import uuid
import json
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import DashScopeEmbeddings
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document

class RAGService:
    def __init__(
        self,
        docs_path: str = "knowledge_base/docs",
        persist_dir: str = "chroma_db_rag_text2sql",
        embedding_model_name: str = "BAAI/bge-small-en-v1.5",  # è‹±æ–‡æ¨¡åž‹ï¼ˆå› æ•°æ®åº“å†…å®¹ä¸ºè‹±æ–‡ï¼‰
        chunk_size: int = 2000,      # å¢žå¤§ï¼šé¿å…åˆ‡ç¢Ž schema
        chunk_overlap: int = 100,
        top_k: int = 4,              # å¤šå¬å›žï¼šschema + å¤šä¸ªç¤ºä¾‹
        force_rebuild: bool = False,
        use_rewrite: bool = False,   # âŒ å…³é—­æŸ¥è¯¢é‡å†™ï¼ˆé˜²æ­¢ä¸¢å¤±å®žä½“ï¼‰
        language: str = "en",        # æ•°æ®åº“å†…å®¹ä¸ºè‹±æ–‡
        llm=None  # éœ€ä¼ å…¥ä¸€ä¸ª callable: llm(prompt: str) -> str
    ):
        self.docs_path = Path(docs_path)
        self.persist_dir = persist_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.force_rebuild = force_rebuild
        self.use_rewrite = use_rewrite
        self.language = language
        self.llm = llm

        # åˆå§‹åŒ–ç»„ä»¶
        self._vectordb = None
        self._bm25 = None
        self._bm25_docs = []


        # åŠ è½½ DashScope Embedding æ¨¡åž‹
        # éœ€è®¾ç½®çŽ¯å¢ƒå˜é‡ DASHSCOPE_API_KEY
        self._embedding = DashScopeEmbeddings(
            model="text-embedding-v2",
            dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")
        )

        self.initialize()

    def _load_documents(self) -> List[Document]:
        """åŠ è½½æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡æ¡£"""
        documents = []
        supported_ext = {'.txt', '.pdf', '.docx', '.md'}
        for file_path in self.docs_path.rglob('*'):
            if file_path.suffix.lower() not in supported_ext:
                continue
            try:
                loader = self._get_loader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata["source"] = str(file_path.resolve())
                documents.extend(docs)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path} | é”™è¯¯: {e}")
        return documents

    def _get_loader(self, file_path: Path):
        suffix = file_path.suffix.lower()
        if suffix == '.txt':
            return TextLoader(file_path, encoding='utf-8')
        elif suffix == '.pdf':
            return PyPDFLoader(file_path)
        elif suffix == '.docx':
            return Docx2txtLoader(file_path)
        elif suffix == '.md':
            return UnstructuredMarkdownLoader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")

    def _clean_text(self, text: str) -> str:
        return text.replace('\n\n', '\n').strip()

    def _build_index(self):
        print("ðŸ”„ æ­£åœ¨æž„å»º Text-to-SQL å‘é‡ç´¢å¼•ä¸ŽBM25ç´¢å¼•...")
        documents = self._load_documents()
        if not documents:
            raise ValueError("çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£ï¼")

        # æ–‡æœ¬åˆ‡ç‰‡ï¼ˆå¤§ chunkï¼‰
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        all_chunks = text_splitter.split_documents(documents)
        for doc in all_chunks:
            doc.page_content = self._clean_text(doc.page_content)

        # æž„å»ºå‘é‡æ•°æ®åº“
        self._vectordb = Chroma.from_documents(
            documents=all_chunks,
            embedding=self._embedding,
            persist_directory=self.persist_dir
        )

        # æž„å»º BM25 ç´¢å¼•ï¼ˆè‹±æ–‡ç›´æŽ¥ splitï¼‰
        self._bm25_docs = all_chunks
        tokenized_corpus = [doc.page_content.lower().split() for doc in all_chunks]
        self._bm25 = BM25Okapi(tokenized_corpus)

        print(f"âœ… Text-to-SQL ç´¢å¼•æž„å»ºå®Œæˆï¼å…± {len(all_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    def initialize(self):
        db_exists = os.path.exists(self.persist_dir)
        if self.force_rebuild or not db_exists:
            if db_exists:
                shutil.rmtree(self.persist_dir)
            self._build_index()
        else:
            self._vectordb = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self._embedding
            )
            # é‡å»º BM25ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            documents = self._load_documents()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
            all_chunks = text_splitter.split_documents(documents)
            for doc in all_chunks:
                doc.page_content = self._clean_text(doc.page_content)
            self._bm25_docs = all_chunks
            tokenized_corpus = [doc.page_content.lower().split() for doc in all_chunks]
            self._bm25 = BM25Okapi(tokenized_corpus)

    def _rewrite_query(self, query: str) -> List[str]:
        # å·²å…³é—­ï¼Œç›´æŽ¥è¿”å›žåŽŸæŸ¥è¯¢
        return [query]

    def _hybrid_search(self, query: str, k: int = 5) -> List[Document]:
        """æ··åˆæ£€ç´¢ï¼šå‘é‡ + BM25ï¼Œä½¿ç”¨ RRF èžåˆæŽ’åº"""
        queries = self._rewrite_query(query)

        # å‘é‡æ£€ç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
        vector_results_with_scores = []
        for q in queries:
            results = self._vectordb.similarity_search_with_score(q, k=k*2)
            vector_results_with_scores.extend(results)
        
        # æŒ‰å‘é‡ç›¸ä¼¼åº¦æŽ’åºï¼Œå»ºç«‹æŽ’å
        vector_results_with_scores.sort(key=lambda x: x[1])  # åˆ†æ•°è¶Šå°è¶Šç›¸ä¼¼
        vector_rank = {self._doc_key(doc): rank for rank, (doc, _) in enumerate(vector_results_with_scores)}

        # BM25 æ£€ç´¢
        bm25_scores = self._bm25.get_scores(query.lower().split())
        top_indices = np.argsort(bm25_scores)[::-1][:k*2]
        bm25_rank = {self._doc_key(self._bm25_docs[i]): rank for rank, i in enumerate(top_indices)}

        # RRF èžåˆï¼ˆReciprocal Rank Fusionï¼‰
        rrf_k = 60  # RRF å¸¸æ•°
        all_docs = {}
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
        for doc, _ in vector_results_with_scores:
            key = self._doc_key(doc)
            if key not in all_docs:
                all_docs[key] = doc
        for i in top_indices:
            doc = self._bm25_docs[i]
            key = self._doc_key(doc)
            if key not in all_docs:
                all_docs[key] = doc
        
        # è®¡ç®— RRF åˆ†æ•°
        rrf_scores = []
        for key, doc in all_docs.items():
            score = 0.0
            if key in vector_rank:
                score += 1.0 / (rrf_k + vector_rank[key])
            if key in bm25_rank:
                score += 1.0 / (rrf_k + bm25_rank[key])
            rrf_scores.append((doc, score))
        
        # æŒ‰ RRF åˆ†æ•°æŽ’åº
        rrf_scores.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in rrf_scores[:k]]
    
    def _doc_key(self, doc: Document) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†"""
        return f"{doc.metadata.get('source', '')}:{doc.page_content[:100]}"

    def sync_with_local_files(self):
        if not self._vectordb:
            return False
        try:
            collection = self._vectordb._collection
            all_metadatas = collection.get(include=["metadatas"])["metadatas"]
            ids_to_delete = []
            for idx, meta in enumerate(all_metadatas):
                source = meta.get("source")
                if source and not os.path.exists(source):
                    ids_to_delete.append(collection.get()["ids"][idx])
            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                print(f"ðŸ—‘ï¸ å·²åˆ é™¤ {len(ids_to_delete)} ä¸ªå¤±æ•ˆæ–‡æ¡£çš„å‘é‡")
            return True
        except Exception as e:
            print(f"Sync failed: {e}")
            return False

    def rebuild_index(self):
        self.force_rebuild = True
        try:
            self.initialize()
            return True
        except Exception as e:
            print(f"Rebuild failed: {e}")
            return False

    def get_indexed_files(self) -> List[str]:
        if not self._vectordb:
            return []
        metadatas = self._vectordb._collection.get(include=["metadatas"])["metadatas"]
        return list(set(meta.get("source") for meta in metadatas if meta.get("source")))

    def ask_sql(self, query: str) -> str:
        """Text-to-SQL ä¸“ç”¨æŽ¥å£"""
        if not query.strip():
            return "SELECT 'è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜';"
        if not self._vectordb:
            return "SELECT 'RAGæœåŠ¡æœªåˆå§‹åŒ–';"

        retrieved_docs = self._hybrid_search(query, k=self.top_k)
        context = "\n".join([doc.page_content for doc in retrieved_docs])

        prompt = f"""You are an expert Text-to-SQL system for a university database.
Convert the user's question into a correct and executable SQL query using SQLite syntax.

Database Schema and Examples:
{context}

Instructions:
- Output ONLY the SQL query, no explanation.
- Use exact table and column names from the schema (e.g., 'dept_name', 'tot_cred').
- For text values, use single quotes (e.g., 'Comp. Sci.').
- If unsure, return: SELECT 'æ— æ³•ç”ŸæˆSQL';
- Do not hallucinate tables or columns.

Question: {query}
SQL:"""

        try:
            sql = self.llm(prompt).strip()
            if not sql.upper().startswith("SELECT") and "æ— æ³•ç”ŸæˆSQL" not in sql:
                return "SELECT 'ç”Ÿæˆçš„SQLæ— æ•ˆ';"
            return sql
        except Exception as e:
            return f"SELECT 'LLMè°ƒç”¨å¤±è´¥: {str(e)}';"
    
    # ======================
    # ðŸ”¹ æ–°å¢žï¼šæ·»åŠ å•ä¸ªæ–‡æ¡£
    # ======================
    def add_document_from_file(self, file_path: str, doc_id: str = None) -> str:
        """
        ä»Žæ–‡ä»¶è·¯å¾„æ·»åŠ ä¸€ä¸ªæ–°æ–‡æ¡£åˆ°å‘é‡åº“å’ŒBM25ç´¢å¼•
        :param file_path: æ–‡ä»¶ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„
        :param doc_id: å¯é€‰ï¼Œè‹¥æœªæä¾›åˆ™è‡ªåŠ¨ç”ŸæˆUUID
        :return: å®žé™…ä½¿ç”¨çš„ doc_id
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_path = Path(file_path)
        suffix = file_path.suffix.lower()
        if suffix not in {'.txt', '.pdf', '.docx', '.md'}:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")

        # ç”Ÿæˆæˆ–ä½¿ç”¨æŒ‡å®š doc_id
        actual_doc_id = doc_id or str(uuid.uuid4())

        # åŠ è½½æ–‡æ¡£
        loader = self._get_loader(file_path)
        documents = loader.load()

        # æ·»åŠ ç»Ÿä¸€å…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update({
                "doc_id": actual_doc_id,
                "original_file": str(file_path.name),
                "source_type": "dynamic_upload"
            })

        # åˆ‡å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = text_splitter.split_documents(documents)
        for chunk in chunks:
            chunk.page_content = self._clean_text(chunk.page_content)

        # æ·»åŠ åˆ°å‘é‡åº“
        self._vectordb.add_documents(chunks)

        # åˆå¹¶åˆ° BM25 æ–‡æ¡£åˆ—è¡¨
        self._bm25_docs.extend(chunks)
        tokenized_corpus = [doc.page_content.lower().split() for doc in self._bm25_docs]
        self._bm25 = BM25Okapi(tokenized_corpus)

        print(f"âœ… å·²æ·»åŠ æ–‡æ¡£ '{file_path.name}' (doc_id={actual_doc_id})")
        return actual_doc_id

    # ======================
    # ðŸ”¹ æ–°å¢žï¼šåˆ é™¤æŒ‡å®šæ–‡æ¡£
    # ======================
    def remove_document(self, doc_id: str) -> bool:
        """
        ä»Žå‘é‡åº“å’ŒBM25ä¸­åˆ é™¤æŒ‡å®š doc_id çš„æ‰€æœ‰ chunks
        :param doc_id: æ–‡æ¡£å”¯ä¸€ID
        :return: æ˜¯å¦åˆ é™¤æˆåŠŸï¼ˆè‡³å°‘åˆ é™¤1ä¸ªchunkï¼‰
        """
        if not self._vectordb:
            return False

        collection = self._vectordb._collection

        # æ­¥éª¤1: ä»Žå‘é‡åº“ä¸­æŸ¥æ‰¾å¹¶åˆ é™¤
        try:
            results = collection.get(where={"doc_id": doc_id}, include=[])
            ids_to_delete = results["ids"]
            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                print(f"ðŸ—‘ï¸ ä»Žå‘é‡åº“åˆ é™¤ {len(ids_to_delete)} ä¸ª chunks (doc_id={doc_id})")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ° doc_id={doc_id} çš„å‘é‡")
        except Exception as e:
            print(f"âŒ å‘é‡åº“åˆ é™¤å¤±è´¥: {e}")
            return False

        # æ­¥éª¤2: ä»Ž BM25 æ–‡æ¡£åˆ—è¡¨ä¸­ç§»é™¤
        before_count = len(self._bm25_docs)
        self._bm25_docs = [
            doc for doc in self._bm25_docs
            if doc.metadata.get("doc_id") != doc_id
        ]
        after_count = len(self._bm25_docs)
        removed_count = before_count - after_count

        # é‡å»º BM25 ç´¢å¼•
        if removed_count > 0:
            tokenized_corpus = [doc.page_content.lower().split() for doc in self._bm25_docs]
            self._bm25 = BM25Okapi(tokenized_corpus)
            print(f"ðŸ§¹ ä»Ž BM25 ç§»é™¤ {removed_count} ä¸ª chunks")

        return (len(ids_to_delete) > 0) or (removed_count > 0)