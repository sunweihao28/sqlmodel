
from google import genai
from openai import OpenAI
import os
import json
from typing import List, Dict, Optional, Iterator, Any
from services.tools import TOOLS_MAP, TOOLS_FUNCTIONS, execute_tool
from services.rag_service import rag_service_instance # Import RAG

# é»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ Key
DEFAULT_API_KEY = os.environ.get("GEMINI_API_KEY")

def _call_llm(prompt: str, model_name: str = 'gpt-4o', api_key: str = None, base_url: str = None) -> str:
    # ... existing implementation ...
    try:
        if base_url:
            client = OpenAI(api_key=api_key or "sk-dummy", base_url=base_url)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        else:
            key_to_use = api_key if api_key else DEFAULT_API_KEY
            if not key_to_use:
                raise ValueError("API Key is missing for Gemini.")
            
            client = genai.Client(api_key=key_to_use)
            response = client.models.generate_content(
                model=model_name if "gemini" in model_name else 'gemini-2.5-flash',
                contents=prompt
            )
            return response.text
    except Exception as e:
        print(f"LLM Call Error ({model_name}): {e}")
        return f"LLM Error: {str(e)}"

# ... existing SQL generation functions (generate_sql_from_text, fix_sql_query, etc.) ...

def generate_sql_from_text(question: str, history: List[Dict], schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    # ... existing code ...
    history_text = ""
    if history:
        history_text = "CONVERSATION HISTORY:\n"
        for msg in history[-5:]: 
            role = "User" if msg['role'] == 'user' else "Assistant"
            content = msg.get('content', '')
            history_text += f"{role}: {content}\n"
    
    prompt = f"""
    You are an expert SQLite Data Analyst. 
    Given the database schema and conversation history, write a valid SQL query to answer the user's *current* question.
    
    SCHEMA:
    {schema}
    
    {history_text}
    
    CURRENT USER QUESTION: "{question}"
    
    INSTRUCTIONS:
    1. Return ONLY the SQL query. Do not include markdown formatting (like ```sql), do not include explanations.
    2. Use SQLite compatible syntax.
    3. If the user asks for a chart or visualization, just select the data needed for it.
    4. If the question cannot be answered by the schema, return SELECT 'I cannot answer this question based on the data' as message;
    """
    
    response = _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)
    return _clean_sql(response)

def fix_sql_query(bad_sql: str, error_msg: str, schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    # ... existing code ...
    prompt = f"""
    You are a SQL debugging expert. 
    I tried to execute a query on this SQLite database, but it failed.
    
    SCHEMA:
    {schema}
    
    FAILED QUERY:
    {bad_sql}
    
    ERROR MESSAGE:
    {error_msg}
    
    INSTRUCTION:
    1. Analyze the error and the schema.
    2. Correct the SQL query to fix the error.
    3. Return ONLY the corrected SQL query. No text.
    """
    
    response = _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)
    return _clean_sql(response)

def generate_analysis(question: str, data: list, api_key: str = None, base_url: str = None, model: str = None) -> str:
    # ... existing code ...
    data_preview = str(data[:20]) 
    prompt = f"""
    User asked: "{question}"
    Data retrieved (first 20 rows): {data_preview}
    
    Provide a very brief (2 sentences) summary of this data in Chinese (Simplified).
    """
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def generate_schema_summary(schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    # ... existing code ...
    prompt = f"""
    You are a helpful Data Assistant.
    A user has just uploaded a new SQLite database file.

    Here is the SCHEMA of the database:
    {schema}

    Please provide a friendly summary of this database.
    1. Tell the user what is your model name(gpt-4o, gemini-2.5-flash, or others).
    2. Briefly explain what this database seems to be about (based on table names).
    3. List the main tables and their key fields (in bullet points).
    4. Suggest 3 interesting questions the user could ask about this data.

    Output format: Markdown.
    Language: Chinese (Simplified) .
    """
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def generate_schema_summary_stream(schema: str, api_key: str = None, base_url: str = None, model: str = None) -> Iterator[str]:
    # ... existing code ...
    prompt = f"""
    You are a helpful Data Assistant.
    A user has just uploaded a new SQLite database file.

    Here is the SCHEMA of the database:
    {schema}

    Please provide a friendly summary of this database.
    1. Tell the user what is your model name(gpt-4o, gemini-2.5-flash, or others).
    2. Briefly explain what this database seems to be about (based on table names).
    3. List the main tables and their key fields (in bullet points).
    4. Suggest 3 interesting questions the user could ask about this data.

    Output format: Markdown.
    Language: Chinese (Simplified).
    """

    if base_url:
        yield from _stream_openai_compatible(prompt, model or 'gpt-4o', api_key, base_url)
    else:
        yield from _stream_gemini(prompt, model or 'gemini-2.5-flash', api_key)

def summarize_user_history(history_text: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    """
    ç”Ÿæˆç”¨æˆ·çš„é•¿æœŸè®°å¿†/ç”»åƒæ‘˜è¦
    """
    prompt = f"""
è¯·é˜…è¯»ä»¥ä¸‹çš„å†å²å¯¹è¯è®°å½•ï¼Œå¹¶å°†å…¶æµ“ç¼©ä¸ºä¸€ä¸ªç®€æ´çš„ç”¨æˆ·ç”»åƒ/æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æå–ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼ˆå¦‚å–œæ¬¢çš„å›¾è¡¨ç±»å‹ã€å…³æ³¨çš„æ•°æ®é¢†åŸŸï¼‰ã€‚
2. æå–ç”¨æˆ·ç»å¸¸æŸ¥è¯¢çš„å…³é”®ä¸šåŠ¡æŒ‡æ ‡æˆ–ç»“è®ºã€‚
3. çœç•¥æ—¥å¸¸å¯’æš„å’Œéå¿…è¦çš„å¯¹è¯ç»†èŠ‚ã€‚
4. è¾“å‡ºä¸€æ®µè¿è´¯çš„æ–‡æœ¬ï¼Œä½œä¸ºåç»­å¯¹è¯çš„"é•¿æœŸè®°å¿†"èƒŒæ™¯ã€‚
5. ä¸è¦æ·»åŠ ä»»ä½•å¼€åœºç™½æˆ–ç»“æŸè¯­ï¼Œç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ã€‚

å†å²è®°å½•å†…å®¹ï¼š
{history_text}
"""
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def _stream_openai_compatible(prompt: str, model: str, api_key: str, base_url: str) -> Iterator[str]:
    # ... existing code ...
    try:
        client = OpenAI(api_key=api_key, base_url=base_url)
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            temperature=0.7,
        )
        for chunk in stream:
            if not chunk or not hasattr(chunk, 'choices') or not chunk.choices: continue
            choice = chunk.choices[0] if len(chunk.choices) > 0 else None
            if not choice or not hasattr(choice, 'delta'): continue
            delta = choice.delta
            if delta and hasattr(delta, 'content') and delta.content:
                yield delta.content
    except Exception as e:
        print(f"OpenAI compatible stream error: {str(e)}")
        yield f"Error: {str(e)}"

def _stream_gemini(prompt: str, model: str, api_key: str) -> Iterator[str]:
    # ... existing code ...
    try:
        key_to_use = api_key or os.environ.get("GEMINI_API_KEY")
        if not key_to_use:
            yield "Error: API Key is missing for Gemini."
            return
        client = genai.Client(api_key=key_to_use)
        response = client.models.generate_content_stream(
            model=model if "gemini" in model else 'gemini-2.5-flash',
            contents=prompt
        )
        for chunk in response:
            if chunk and hasattr(chunk, 'text') and chunk.text:
                yield chunk.text
    except Exception as e:
        print(f"Gemini stream error: {str(e)}")
        yield f"Error: {str(e)}"

def _clean_sql(text: str) -> str:
    # ... existing code ...
    if not text: return ""
    sql = text.strip()
    if sql.startswith("```"):
        lines = sql.split('\n')
        if lines[0].startswith("```"): lines = lines[1:]
        if lines and lines[-1].startswith("```"): lines = lines[:-1]
        sql = "\n".join(lines).strip()
    return sql

def agent_analyze_database_stream(
    question: str,
    schema: str,
    db_path: str = None, 
    connection_url: str = None, 
    history: List[Dict] = None,
    api_key: str = None,
    base_url: str = None,
    model: str = None,
    max_tool_rounds: int = 12,
    use_rag: bool = False,
    allow_auto_execute: bool = False,
    user_memory: str = None # [New Param]
) -> Iterator[Dict[str, Any]]:
    """
    æµå¼Agentæ¨ç†å‡½æ•°
    """
    # 1. RAG Context
    rag_context = ""
    if use_rag:
        try:
            docs = rag_service_instance.hybrid_search(
                question, 
                api_key=api_key, 
                base_url=base_url
            )
            if docs:
                rag_context = "\n\nã€çŸ¥è¯†åº“å‚è€ƒä¿¡æ¯ (RAG Retrieval)ã€‘:\n"
                for i, doc in enumerate(docs):
                    rag_context += f"æ–‡æ¡£ç‰‡æ®µ {i+1} (æ¥æº: {doc.metadata.get('original_file', 'unknown')}):\n{doc.page_content}\n---\n"
                
                yield {"type": "text", "content": f"ğŸ“š å·²æ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³çŸ¥è¯†åº“æ–‡æ¡£...\n\n"}
        except Exception as e:
            print(f"RAG search error: {e}")
            yield {"type": "error", "error": f"RAGæ£€ç´¢å¤±è´¥: {str(e)}"}

    # 2. Memory Context [New]
    memory_context = ""
    if user_memory:
        memory_context = f"\n\nã€ç”¨æˆ·é•¿æœŸè®°å¿†/ç”»åƒ (User Memory)ã€‘:\n{user_memory}\nè¯·åŸºäºæ­¤ç”»åƒäº†è§£ç”¨æˆ·çš„åå¥½å’Œå…³æ³¨ç‚¹ã€‚\n"
        yield {"type": "text", "content": f"ğŸ§  å·²åŠ è½½ç”¨æˆ·é•¿æœŸè®°å¿†...\n\n"}

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if base_url:
        client = OpenAI(api_key=api_key or "sk-dummy", base_url=base_url)
    else:
        key_to_use = api_key or os.environ.get("GEMINI_API_KEY")
        if not key_to_use:
            yield {"type": "error", "error": "API Key is missing."}
            return
        client = OpenAI(api_key=key_to_use)
    
    # æ ¼å¼åŒ–å†å²è®°å½•
    history_text = ""
    if history:
        history_text = "\nCONVERSATION HISTORY:\n"
        for msg in history[-5:]:
            role = "User" if msg.get('role') == 'user' else "Assistant"
            content = msg.get('content', '')
            history_text += f"{role}: {content}\n"
    
    # æ„å»ºç³»ç»Ÿæç¤º (æ³¨å…¥ RAG Context å’Œ Memory Context)
    system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨SQLå’ŒPythonè¿›è¡Œæ•°æ®åˆ†æã€‚

æ•°æ®åº“Schemaä¿¡æ¯:
{schema}

{rag_context}
{memory_context}

å¯ç”¨å·¥å…·:
1. sql_inter: æ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆcolumns, rows, row_countï¼‰
2. extract_data: å°†SQLæŸ¥è¯¢ç»“æœåŠ è½½åˆ°pandas DataFrameä¾›Pythonä½¿ç”¨
3. python_inter: æ‰§è¡ŒPythonä»£ç è¿›è¡Œæ•°æ®å¤„ç†ã€åˆ†æå’Œå¯è§†åŒ–é…ç½®ç”Ÿæˆ

å¯è§†åŒ–è¯´æ˜:
- å¦‚éœ€ç”Ÿæˆå›¾è¡¨ï¼Œåœ¨Pythonä»£ç ä¸­åˆ›å»º 'visualization_config' å­—å…¸å˜é‡
- é…ç½®æ ¼å¼ï¼š{{"type": "bar|line|pie|table", "title": "...", "xAxis": {{"key": "..."}}, "data": [...]}}
- å‰ç«¯ä¼šæ ¹æ®é…ç½®è‡ªåŠ¨æ¸²æŸ“å›¾è¡¨ï¼Œæ— éœ€ä½¿ç”¨matplotlib

å·¥ä½œæµç¨‹:
- æ ¹æ®ç”¨æˆ·é—®é¢˜{ "ã€å‚è€ƒçš„çŸ¥è¯†åº“ä¿¡æ¯" if rag_context else "" }{ "åŠç”¨æˆ·é•¿æœŸè®°å¿†" if user_memory else "" }ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œåˆ†æ
- å¯ä»¥è¿ç»­å¤šæ¬¡è°ƒç”¨å·¥å…·
- SQLæŸ¥è¯¢ä¼šè‡ªåŠ¨æ·»åŠ LIMIT 50é™åˆ¶
- å¦‚æœSQLæ‰§è¡Œå¤±è´¥ï¼Œåˆ†æé”™è¯¯ä¿¡æ¯å¹¶å°è¯•ä¿®å¤

é‡è¦è¦æ±‚:
- ä¼˜å…ˆå‚è€ƒçŸ¥è¯†åº“ä¸­çš„ä¸šåŠ¡å®šä¹‰ã€æŒ‡æ ‡è®¡ç®—å…¬å¼æˆ–å­—æ®µè¯´æ˜ã€‚
- **æœ€ç»ˆå›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡(Simplified Chinese)**ã€‚
- å¦‚æœéœ€è¦ç¡®è®¤æ‰§è¡ŒSQLï¼Œè¯·ç”Ÿæˆç›¸åº”çš„å·¥å…·è°ƒç”¨ã€‚
"""
    
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    
    if history_text:
        messages.append({"role": "user", "content": history_text})
    
    messages.append({"role": "user", "content": question})
    
    tools = [{"type": "function", "function": tool_def} for tool_def in TOOLS_MAP]
    
    tool_rounds = 0
    
    while tool_rounds < max_tool_rounds:
        tool_rounds += 1
        
        try:
            try:
                response = client.chat.completions.create(
                    model=model or ('gpt-4o' if base_url else 'gemini-2.5-flash'),
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    stream=True,
                )
            except Exception as api_error:
                error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {type(api_error).__name__}: {str(api_error)}"
                yield {"type": "error", "error": error_msg}
                return
            
            response_message_content = ""
            tool_calls = []
            
            for chunk in response:
                try:
                    if not chunk or not hasattr(chunk, 'choices') or not chunk.choices: continue
                    if len(chunk.choices) == 0: continue
                    choice = chunk.choices[0]
                    if not choice or not hasattr(choice, 'delta'): continue
                    delta = choice.delta
                    if not delta: continue
                    
                    if hasattr(delta, 'content') and delta.content:
                        response_message_content += delta.content
                        yield {"type": "text", "content": delta.content}
                    
                    if hasattr(delta, 'tool_calls') and delta.tool_calls:
                        for tc_delta in delta.tool_calls:
                            if not hasattr(tc_delta, 'index'): continue
                            idx = tc_delta.index
                            if idx >= len(tool_calls):
                                tool_calls.extend([None] * (idx + 1 - len(tool_calls)))
                            if tool_calls[idx] is None:
                                tool_calls[idx] = {
                                    "id": getattr(tc_delta, 'id', '') or "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                }
                            if hasattr(tc_delta, 'function') and tc_delta.function:
                                if hasattr(tc_delta.function, 'name') and tc_delta.function.name:
                                    tool_calls[idx]["function"]["name"] = tc_delta.function.name
                                if hasattr(tc_delta.function, 'arguments') and tc_delta.function.arguments:
                                    tool_calls[idx]["function"]["arguments"] += tc_delta.function.arguments
                except Exception as e:
                    continue
            
            valid_tool_calls = [tc for tc in tool_calls if tc is not None and tc.get("function", {}).get("name")]
            
            if not valid_tool_calls:
                if not response_message_content:
                    yield {"type": "text", "content": "åˆ†æå®Œæˆã€‚"}
                yield {"type": "done"}
                return
            
            assistant_msg = {
                "role": "assistant",
                "content": response_message_content,
                "tool_calls": valid_tool_calls
            }
            messages.append(assistant_msg)
            
            for tool_call in valid_tool_calls:
                function_name = tool_call["function"]["name"]
                function_args_str = tool_call["function"]["arguments"]
                
                try:
                    function_args = json.loads(function_args_str)
                except json.JSONDecodeError:
                    if function_name == "python_inter": function_args = {"py_code": function_args_str}
                    elif function_name == "sql_inter": function_args = {"sql_query": function_args_str}
                    elif function_name == "extract_data": function_args = {"sql_query": function_args_str, "df_name": "df"}
                    else: function_args = {}
                
                sql_code = None
                if function_name == "sql_inter" and "sql_query" in function_args:
                    sql_code = function_args["sql_query"]
                if function_name == "extract_data" and "sql_query" in function_args:
                    sql_code = function_args["sql_query"]
                
                # Intercept SQL execution OR Data Extraction if auto_execute is False
                if function_name in ("sql_inter", "extract_data") and not allow_auto_execute:
                    yield {
                        "type": "tool_call",
                        "tool": function_name,
                        "status": "pending_approval",
                        "sql_code": sql_code
                    }
                    yield {"type": "done"}
                    return
                
                # Normal execution
                tool_call_event = {"type": "tool_call", "tool": function_name, "status": "start"}
                if sql_code: tool_call_event["sql_code"] = sql_code
                yield tool_call_event
                
                try:
                    session_id = db_path if db_path else "remote_db"
                    
                    if function_name in ("sql_inter", "extract_data"):
                        result = execute_tool(
                            function_name, 
                            function_args, 
                            db_path=db_path, 
                            connection_url=connection_url,
                            session_id=session_id
                        )
                    else:
                        result = execute_tool(function_name, function_args, session_id=session_id)
                    
                    yield {
                        "type": "tool_result",
                        "tool": function_name,
                        "result": result,
                        "status": "success"
                    }
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "name": function_name,
                        "content": result
                    })
                except Exception as e:
                    error_msg = f"Error ({function_name}): {str(e)}"
                    yield {
                        "type": "tool_result",
                        "tool": function_name,
                        "result": error_msg,
                        "status": "error"
                    }
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "name": function_name,
                        "content": error_msg
                    })
            
        except Exception as e:
            error_detail = f"Process Error: {str(e)}"
            yield {"type": "error", "error": error_detail}
            yield {"type": "done"}
            return
    
    yield {"type": "error", "error": "Max tool rounds reached."}
    yield {"type": "done"}