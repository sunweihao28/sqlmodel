
from google import genai
from google.genai import types
from openai import OpenAI
import os
import json
import time
from typing import List, Dict, Optional, Iterator, Any
from services.tools import TOOLS_MAP, TOOLS_FUNCTIONS, execute_tool
from services.rag_service import rag_service_instance  # Import RAG
from services.enhanced_sql import generate_sql_enhanced

# åŠ è½½ç¯å¢ƒå˜é‡ä¸­çš„ Key
DEFAULT_GEMINI_KEY = os.environ.get("GEMINI_API_KEY")
DEFAULT_OPENAI_KEY = os.environ.get("OPENAI_API_KEY")

def _should_use_gemini(model_name: str, base_url: str = None) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ Google GenAI åŸç”Ÿå®¢æˆ·ç«¯ã€‚
    è§„åˆ™ï¼š
    1. å¦‚æœæä¾›äº† base_urlï¼Œé€šå¸¸æ˜¯ OpenAI å…¼å®¹æ¥å£ï¼ˆDeepSeek, OneAPIç­‰ï¼‰ï¼Œè¿”å› Falseã€‚
    2. å¦‚æœæ²¡æœ‰ base_urlï¼Œä¸”æ¨¡å‹ååŒ…å« 'gemini'ï¼Œè¿”å› Trueã€‚
    3. å…¶ä»–æƒ…å†µï¼ˆå¦‚ gpt-4o ä¸”æ—  base_urlï¼‰ï¼Œé»˜è®¤ä½¿ç”¨ OpenAI å®˜æ–¹æ¥å£ï¼Œè¿”å› Falseã€‚
    """
    if base_url:
        return False
    if model_name and "gemini" in model_name.lower():
        return True
    return False

def _call_llm(prompt: str, model_name: str = 'gpt-4o', api_key: str = None, base_url: str = None) -> str:
    try:
        use_gemini = _should_use_gemini(model_name, base_url)
        
        if not use_gemini:
            # OpenAI / Compatible
            key = api_key or DEFAULT_OPENAI_KEY
            if not key and not base_url:
                print(f"Warning: No API Key found for OpenAI model {model_name}")
            
            client = OpenAI(api_key=key or "sk-dummy", base_url=base_url)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        else:
            # Gemini Native
            key = api_key or DEFAULT_GEMINI_KEY
            if not key:
                raise ValueError("API Key is missing for Gemini.")
            
            client = genai.Client(api_key=key)
            response = client.models.generate_content(
                model=model_name,
                contents=prompt
            )
            return response.text
    except Exception as e:
        print(f"LLM Call Error ({model_name}): {e}")
        return f"LLM Error: {str(e)}"

def generate_sql_from_text(question: str, history: List[Dict], schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    history_text = ""
    if history:
        history_text = "CONVERSATION HISTORY:\n"
        for msg in history[-5:]: 
            role = "User" if msg['role'] == 'user' else "Assistant"
            content = msg.get('content', '')
            history_text += f"{role}: {content}\n"
    
    prompt = f"""
    You are an expert SQLite Data Analyst. 
    Given the database schema and conversation history, write a valid SQL query to answer the user's *current* question.
    
    SCHEMA:
    {schema}
    
    {history_text}
    
    CURRENT USER QUESTION: "{question}"
    
    INSTRUCTIONS:
    1. Return ONLY the SQL query. Do not include markdown formatting (like ```sql), do not include explanations.
    2. Use SQLite compatible syntax.
    3. If the user asks for a chart or visualization, just select the data needed for it.
    4. If the question cannot be answered by the schema, return SELECT 'I cannot answer this question based on the data' as message;
    """
    
    response = _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)
    return _clean_sql(response)

def fix_sql_query(bad_sql: str, error_msg: str, schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    prompt = f"""
    You are a SQL debugging expert. 
    I tried to execute a query on this SQLite database, but it failed.
    
    SCHEMA:
    {schema}
    
    FAILED QUERY:
    {bad_sql}
    
    ERROR MESSAGE:
    {error_msg}
    
    INSTRUCTION:
    1. Analyze the error and the schema.
    2. Correct the SQL query to fix the error.
    3. Return ONLY the corrected SQL query. No text.
    """
    
    response = _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)
    return _clean_sql(response)

def generate_analysis(question: str, data: list, api_key: str = None, base_url: str = None, model: str = None) -> str:
    data_preview = str(data[:20]) 
    prompt = f"""
    User asked: "{question}"
    Data retrieved (first 20 rows): {data_preview}
    
    Provide a very brief (2 sentences) summary of this data in Chinese (Simplified).
    """
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def generate_schema_summary(schema: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    prompt = f"""
    You are a helpful Data Assistant.
    A user has just uploaded a new SQLite database file.

    Here is the SCHEMA of the database:
    {schema}

    Please provide a friendly summary of this database.
    1. Tell the user what is your model name(gpt-4o, gemini-2.5-flash, or others).
    2. Briefly explain what this database seems to be about (based on table names).
    3. List the main tables and their key fields (in bullet points).
    4. Suggest 3 interesting questions the user could ask about this data.

    Output format: Markdown.
    Language: Chinese (Simplified) .
    """
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def generate_schema_summary_stream(schema: str, api_key: str = None, base_url: str = None, model: str = None) -> Iterator[str]:
    prompt = f"""
    You are a helpful Data Assistant.
    A user has just uploaded a new SQLite database file.

    Here is the SCHEMA of the database:
    {schema}

    Please provide a friendly summary of this database.
    1. Tell the user what is your model name(gpt-4o, gemini-2.5-flash, or others).
    2. Briefly explain what this database seems to be about (based on table names).
    3. List the main tables and their key fields (in bullet points).
    4. Suggest 3 interesting questions the user could ask about this data.

    Output format: Markdown.
    Language: Chinese (Simplified).
    """

    use_gemini = _should_use_gemini(model, base_url)
    
    if not use_gemini:
        yield from _stream_openai_compatible(prompt, model or 'gpt-4o', api_key, base_url)
    else:
        yield from _stream_gemini(prompt, model or 'gemini-2.5-flash', api_key)

def summarize_user_history(history_text: str, api_key: str = None, base_url: str = None, model: str = None) -> str:
    prompt = f"""
è¯·é˜…è¯»ä»¥ä¸‹çš„å†å²å¯¹è¯è®°å½•ï¼Œå¹¶å°†å…¶æµ“ç¼©ä¸ºä¸€ä¸ªç®€æ´çš„ç”¨æˆ·ç”»åƒ/æ‘˜è¦ã€‚
å†å²è®°å½•å†…å®¹ï¼š
{history_text}
"""
    return _call_llm(prompt, model or 'gemini-2.5-flash', api_key, base_url)

def _stream_openai_compatible(prompt: str, model: str, api_key: str, base_url: str) -> Iterator[str]:
    try:
        key = api_key or DEFAULT_OPENAI_KEY
        client = OpenAI(api_key=key or "sk-dummy", base_url=base_url)
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            temperature=0.7,
        )
        for chunk in stream:
            if not chunk or not hasattr(chunk, 'choices') or not chunk.choices: continue
            choice = chunk.choices[0] if len(chunk.choices) > 0 else None
            if not choice or not hasattr(choice, 'delta'): continue
            delta = choice.delta
            if delta and hasattr(delta, 'content') and delta.content:
                yield delta.content
    except Exception as e:
        print(f"OpenAI compatible stream error: {str(e)}")
        yield f"Error: {str(e)}"

def _stream_gemini(prompt: str, model: str, api_key: str) -> Iterator[str]:
    try:
        key_to_use = api_key or DEFAULT_GEMINI_KEY
        if not key_to_use:
            yield "Error: API Key is missing for Gemini."
            return
        client = genai.Client(api_key=key_to_use)
        response = client.models.generate_content_stream(
            model=model if "gemini" in model else 'gemini-2.5-flash',
            contents=prompt
        )
        for chunk in response:
            if chunk and hasattr(chunk, 'text') and chunk.text:
                yield chunk.text
    except Exception as e:
        print(f"Gemini stream error: {str(e)}")
        yield f"Error: {str(e)}"

def _clean_sql(text: str) -> str:
    if not text: return ""
    sql = text.strip()
    if sql.startswith("```"):
        lines = sql.split('\n')
        if lines[0].startswith("```"): lines = lines[1:]
        if lines and lines[-1].startswith("```"): lines = lines[:-1]
        sql = "\n".join(lines).strip()
    return sql

def agent_analyze_database_stream(
    question: str,
    schema: str,
    db_path: str = None,
    connection_url: str = None,
    history: List[Dict] = None,
    api_key: str = None,
    base_url: str = None,
    model: str = None,
    max_tool_rounds: int = 12,
    use_rag: bool = False,
    allow_auto_execute: bool = True,
    user_memory: str = None,
    use_sql_expert: bool = False,
    user_id: int = None,
) -> Iterator[Dict[str, Any]]:
    """
    æµå¼Agentæ¨ç†å‡½æ•° (Supports both OpenAI and Gemini Native)
    """
    # 1. RAG Context
    rag_context = ""
    if use_rag and user_id: 
        try:
            docs = rag_service_instance.hybrid_search(
                user_id,
                question, 
                api_key=api_key, 
                base_url=base_url
            )
            if docs:
                rag_context = "\n\nã€çŸ¥è¯†åº“å‚è€ƒä¿¡æ¯ (RAG Retrieval)ã€‘:\n"
                for i, doc in enumerate(docs):
                    rag_context += f"æ–‡æ¡£ç‰‡æ®µ {i+1} (æ¥æº: {doc.metadata.get('original_file', 'unknown')}):\n{doc.page_content}\n---\n"
                
                yield {"type": "text", "content": f"ğŸ“š å·²æ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³çŸ¥è¯†åº“æ–‡æ¡£...\n\n"}
        except Exception as e:
            print(f"RAG search error: {e}")
            yield {"type": "error", "error": f"RAGæ£€ç´¢å¤±è´¥: {str(e)}"}

    # 2. Memory Context
    memory_context = ""
    if user_memory:
        memory_context = f"\n\nã€ç”¨æˆ·é•¿æœŸè®°å¿†/ç”»åƒ (User Memory)ã€‘:\n{user_memory}\nè¯·åŸºäºæ­¤ç”»åƒäº†è§£ç”¨æˆ·çš„åå¥½å’Œå…³æ³¨ç‚¹ã€‚\n"
        yield {"type": "text", "content": f"ğŸ§  å·²åŠ è½½ç”¨æˆ·é•¿æœŸè®°å¿†...\n\n"}

    # Determine Provider
    is_gemini = _should_use_gemini(model, base_url)
    
    # Initialize Clients
    client = None
    if not is_gemini:
        key = api_key or DEFAULT_OPENAI_KEY
        client = OpenAI(api_key=key or "sk-dummy", base_url=base_url)
    else:
        key_to_use = api_key or DEFAULT_GEMINI_KEY
        if not key_to_use:
            yield {"type": "error", "error": "API Key is missing for Gemini."}
            return
        client = genai.Client(api_key=key_to_use)
    
    # æ ¼å¼åŒ–å†å²è®°å½•
    history_text = ""
    if history:
        history_text = "\nCONVERSATION HISTORY:\n"
        for msg in history[-5:]:
            role = "User" if msg.get('role') == 'user' else "Assistant"
            content = msg.get('content', '')
            history_text += f"{role}: {content}\n"
    
    # æ„å»ºç³»ç»Ÿæç¤º - æ ¹æ®æ˜¯å¦æœ‰ schema åŒºåˆ†æ¨¡å¼
    if schema:
        # DB Connected Mode
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨SQLå’ŒPythonè¿›è¡Œæ•°æ®åˆ†æã€‚

æ•°æ®åº“Schemaä¿¡æ¯:
{schema}

{rag_context}
{memory_context}

å¯ç”¨å·¥å…·:
1. sql_inter: æ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆcolumns, rows, row_countï¼‰
2. extract_data: å°†SQLæŸ¥è¯¢ç»“æœåŠ è½½åˆ°pandas DataFrameä¾›Pythonä½¿ç”¨
3. python_inter: æ‰§è¡ŒPythonä»£ç è¿›è¡Œæ•°æ®å¤„ç†ã€åˆ†æå’Œå¯è§†åŒ–é…ç½®ç”Ÿæˆ

å¯è§†åŒ–è¯´æ˜ï¼šè‹¥éœ€åœ¨å‰ç«¯å±•ç¤ºå›¾è¡¨æˆ–è¡¨æ ¼ï¼Œåœ¨ Python ä¸­å¿…é¡»å°† visualization_config èµ‹å€¼ä¸ºåˆ—è¡¨ï¼ˆä¸€ä¸ªæˆ–å¤šä¸ªé…ç½®ï¼‰ï¼Œç”±å‰ç«¯æŒ‰é¡ºåºå†…è”æ¸²æŸ“ï¼š
  visualization_config = [
    {{"type": "table", "title": "å›¾è¡¨æ ‡é¢˜", "data": [{{"åˆ—åA": "å€¼1", "åˆ—åB": 100}}, {{"åˆ—åA": "å€¼2", "åˆ—åB": 200}}]}},
    {{"type": "bar", "title": "å¦ä¸€å¼ å›¾", "data": [...]}}
  ]
  type å¯ä¸º "table"/"bar"/"line"/"pie"ã€‚data ä¸ºè¡Œåˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ª dictï¼Œå•å…ƒæ ¼ä»…é™ str/int/float/bool/Noneï¼›ä» DataFrame ç”¨ to_dict(orient='records') æˆ–å…ˆè½¬åŸºæœ¬ç±»å‹ã€‚æ— éœ€ matplotlibã€‚ä»…ç”Ÿæˆå¯è§†åŒ–é…ç½®å³å¯ï¼›å›å¤æ­£æ–‡ä¸­ç¦æ­¢ç”¨ Markdown æˆ–æ–‡å­—å†æ¬¡è¾“å‡ºåŒä¸€ä»½è¡¨æ ¼/å›¾è¡¨æ•°æ®ï¼Œæ€»ç»“æ—¶ç”¨è‡ªç„¶è¡¨è¿°å³å¯ã€‚

å·¥ä½œæµç¨‹:
- æ ¹æ®ç”¨æˆ·é—®é¢˜{ "ã€å‚è€ƒçš„çŸ¥è¯†åº“ä¿¡æ¯" if rag_context else "" }{ "åŠç”¨æˆ·é•¿æœŸè®°å¿†" if user_memory else "" }ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œåˆ†æ
- å¯ä»¥è¿ç»­å¤šæ¬¡è°ƒç”¨å·¥å…·
- SQLæŸ¥è¯¢ä¼šè‡ªåŠ¨æ·»åŠ LIMIT 50é™åˆ¶
- å¦‚æœSQLæ‰§è¡Œå¤±è´¥ï¼Œåˆ†æé”™è¯¯ä¿¡æ¯å¹¶å°è¯•ä¿®å¤

é‡è¦è¦æ±‚:
- ä¼˜å…ˆå‚è€ƒçŸ¥è¯†åº“ä¸­çš„ä¸šåŠ¡å®šä¹‰ã€æŒ‡æ ‡è®¡ç®—å…¬å¼æˆ–å­—æ®µè¯´æ˜ã€‚
- **æœ€ç»ˆå›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡(Simplified Chinese)**ã€‚
- å¦‚æœéœ€è¦ç¡®è®¤æ‰§è¡ŒSQLï¼Œè¯·ç”Ÿæˆç›¸åº”çš„å·¥å…·è°ƒç”¨ã€‚
- è‹¥å·²é€šè¿‡ python_inter çš„ visualization_config ç”Ÿæˆäº†è¡¨æ ¼æˆ–å›¾è¡¨ï¼Œåˆ™**ä¸è¦åœ¨å›å¤æ­£æ–‡ä¸­ç”¨ Markdown è¡¨æ ¼ï¼ˆ|...|ï¼‰æˆ–é€è¡Œæ•°æ®å†æ¬¡åˆ—å‡º**ï¼Œç”¨ç®€çŸ­è‡ªç„¶çš„è¯æ¦‚æ‹¬ç»“è®ºå³å¯ï¼Œä¸è¦å¥—ç”¨å›ºå®šè¯æœ¯ã€‚
"""
    else:
        # General Chat Mode (No DB)
        system_prompt = f"""ä½ æ˜¯ä¸€ä½æ™ºèƒ½åŠ©æ‰‹ã€‚å½“å‰ç”¨æˆ·æœªè¿æ¥ä»»ä½•æ•°æ®åº“ï¼Œå› æ­¤æ— æ³•æ‰§è¡Œ SQL æŸ¥è¯¢æˆ–è®¿é—®æ•°æ®è¡¨ã€‚

{rag_context}
{memory_context}

ä½ å¯ä»¥è¿›è¡Œé€šç”¨å¯¹è¯ã€é€»è¾‘æ¨ç†ã€ä»£ç ç¼–å†™ï¼ˆä½¿ç”¨ python_interï¼‰æˆ–å›ç­”åŸºäºçŸ¥è¯†åº“/é•¿æœŸè®°å¿†çš„é—®é¢˜ã€‚
å¦‚æœç”¨æˆ·è¦æ±‚æŸ¥è¯¢æ•°æ®åº“æ•°æ®ï¼Œè¯·ç¤¼è²Œåœ°æç¤ºç”¨æˆ·å…ˆè¿æ¥æ•°æ®åº“æˆ–ä¸Šä¼ æ–‡ä»¶ã€‚

å¯ç”¨å·¥å…·:
1. python_inter: æ‰§è¡Œé€šç”¨ Python ä»£ç è®¡ç®—æˆ–é€»è¾‘éªŒè¯ã€‚

**æœ€ç»ˆå›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡(Simplified Chinese)**ã€‚
"""

    # Messages structure
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    if history_text:
        messages.append({"role": "user", "content": history_text})
    messages.append({"role": "user", "content": question})
    
    # Tools definition - Filter SQL tools if no schema
    all_tools = [{"type": "function", "function": tool_def} for tool_def in TOOLS_MAP]
    if not schema:
        # Only keep python_inter for general purpose, remove SQL tools
        tools = [t for t in all_tools if t["function"]["name"] == "python_inter"]
    else:
        tools = all_tools
        
    gemini_tools = [t['function'] for t in tools]
    
    tool_rounds = 0
    
    while tool_rounds < max_tool_rounds:
        tool_rounds += 1
        
        # --- Retry Loop for Network Instability ---
        max_retries = 3
        retry_delay = 1
        
        response_message_content = ""
        tool_calls = []
        
        success = False
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # Reset accumulators for this attempt
                response_message_content = ""
                tool_calls = []
                
                if is_gemini:
                    # --- GEMINI NATIVE PATH ---
                    # Convert messages to Gemini Format
                    gemini_contents = []
                    for m in messages:
                        if m['role'] == 'system': continue
                        if m['role'] == 'user':
                            gemini_contents.append(types.Content(role='user', parts=[types.Part(text=m['content'])]))
                        elif m['role'] == 'assistant':
                            parts = []
                            if m.get('content'): parts.append(types.Part(text=m['content']))
                            if m.get('tool_calls'):
                                for tc in m['tool_calls']:
                                    args = {}
                                    try: args = json.loads(tc['function']['arguments'])
                                    except: pass
                                    parts.append(types.Part(function_call=types.FunctionCall(name=tc['function']['name'], args=args)))
                            gemini_contents.append(types.Content(role='model', parts=parts))
                        elif m['role'] == 'tool':
                            gemini_contents.append(types.Content(role='user', parts=[types.Part(
                                function_response=types.FunctionResponse(name=m['name'], response={'result': m['content']})
                            )]))

                    # Prepare config with available tools (might be empty list if no tools allowed, but here we at least have python)
                    gemini_config = types.GenerateContentConfig(
                        system_instruction=system_prompt,
                        temperature=0.1
                    )
                    if gemini_tools:
                         gemini_config.tools = [types.Tool(function_declarations=gemini_tools)]

                    response = client.models.generate_content_stream(
                        model=model or 'gemini-2.5-flash',
                        contents=gemini_contents,
                        config=gemini_config
                    )

                    for chunk in response:
                        if chunk.text:
                            response_message_content += chunk.text
                            yield {"type": "text", "content": chunk.text}
                        if chunk.function_calls:
                            for fc in chunk.function_calls:
                                tool_calls.append({
                                    "id": "gemini_call_id", 
                                    "type": "function",
                                    "function": {
                                        "name": fc.name,
                                        "arguments": json.dumps(fc.args) 
                                    }
                                })
                else:
                    # --- OPENAI COMPATIBLE PATH ---
                    # If no tools available (e.g. extremely restricted mode), don't pass tools param
                    req_kwargs = {
                        "model": model or 'gpt-4o',
                        "messages": messages,
                        "stream": True,
                    }
                    if tools:
                        req_kwargs["tools"] = tools
                        req_kwargs["tool_choice"] = "auto"

                    response = client.chat.completions.create(**req_kwargs)
                    
                    for chunk in response:
                        if not chunk or not hasattr(chunk, 'choices') or not chunk.choices: continue
                        if len(chunk.choices) == 0: continue
                        choice = chunk.choices[0]
                        if not choice or not hasattr(choice, 'delta'): continue
                        delta = choice.delta
                        if not delta: continue
                        
                        if hasattr(delta, 'content') and delta.content:
                            response_message_content += delta.content
                            yield {"type": "text", "content": delta.content}
                        
                        if hasattr(delta, 'tool_calls') and delta.tool_calls:
                            for tc_delta in delta.tool_calls:
                                if not hasattr(tc_delta, 'index'): continue
                                idx = tc_delta.index
                                if idx >= len(tool_calls):
                                    tool_calls.extend([None] * (idx + 1 - len(tool_calls)))
                                if tool_calls[idx] is None:
                                    tool_calls[idx] = {
                                        "id": getattr(tc_delta, 'id', '') or "",
                                        "type": "function",
                                        "function": {"name": "", "arguments": ""}
                                    }
                                if hasattr(tc_delta, 'function') and tc_delta.function:
                                    if hasattr(tc_delta.function, 'name') and tc_delta.function.name:
                                        tool_calls[idx]["function"]["name"] = tc_delta.function.name
                                    if hasattr(tc_delta.function, 'arguments') and tc_delta.function.arguments:
                                        tool_calls[idx]["function"]["arguments"] += tc_delta.function.arguments

                success = True
                break  # Break retry loop on success

            except Exception as e:
                last_error = e
                print(f"LLM Stream Error (Attempt {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    yield {"type": "text", "content": "\nâš ï¸ [ç½‘ç»œæ³¢åŠ¨ï¼Œæ­£åœ¨é‡è¯•...]\n"}
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    # Final attempt failed
                    pass
        
        if not success:
             error_detail = f"Process Error: {str(last_error)}"
             yield {"type": "error", "error": error_detail}
             yield {"type": "done"}
             return

        # --- End of Retry Loop ---

        try:
            # --- COMMON LOGIC: Execute Tools & Update History ---
            
            valid_tool_calls = [tc for tc in tool_calls if tc is not None and tc.get("function", {}).get("name")]
            
            if not valid_tool_calls:
                if not response_message_content:
                    # fallback just in case
                    yield {"type": "text", "content": "åˆ†æå®Œæˆã€‚"}
                yield {"type": "done"}
                return
            
            # Append assistant message to history
            assistant_msg = {
                "role": "assistant",
                "content": response_message_content,
                "tool_calls": valid_tool_calls
            }
            messages.append(assistant_msg)
            
            for tool_call in valid_tool_calls:
                function_name = tool_call["function"]["name"]
                function_args_str = tool_call["function"]["arguments"]
                
                try:
                    function_args = json.loads(function_args_str)
                except json.JSONDecodeError:
                    if function_name == "python_inter": function_args = {"py_code": function_args_str}
                    elif function_name == "sql_inter": function_args = {"sql_query": function_args_str}
                    elif function_name == "extract_data": function_args = {"sql_query": function_args_str, "df_name": "df"}
                    else: function_args = {}
                
                sql_code = None
                if function_name == "sql_inter" and "sql_query" in function_args:
                    sql_code = function_args["sql_query"]
                if function_name == "extract_data" and "sql_query" in function_args:
                    sql_code = function_args["sql_query"]

                # SQL ä¸“å®¶æ¨¡å¼
                if use_sql_expert and db_path and function_name in ("sql_inter", "extract_data"):
                    expert_sql = generate_sql_enhanced(
                        question=question,
                        db_path=db_path,
                        api_key=api_key,
                        base_url=base_url,
                        model=model,
                    )
                    if expert_sql:
                        sql_code = expert_sql
                        function_args = {**function_args, "sql_query": expert_sql}

                # Human-in-the-loop SQL Check
                if function_name in ("sql_inter", "extract_data") and not allow_auto_execute:
                    yield {
                        "type": "tool_call",
                        "tool": function_name,
                        "status": "pending_approval",
                        "sql_code": sql_code
                    }
                    yield {"type": "done"}
                    return
                
                # Yield Tool Call Event
                tool_call_event = {"type": "tool_call", "tool": function_name, "status": "start"}
                if sql_code: tool_call_event["sql_code"] = sql_code
                yield tool_call_event
                
                try:
                    session_id = db_path if db_path else "remote_db"
                    
                    if function_name in ("sql_inter", "extract_data"):
                        result = execute_tool(
                            function_name, 
                            function_args, 
                            db_path=db_path, 
                            connection_url=connection_url,
                            session_id=session_id
                        )
                    else:
                        result = execute_tool(function_name, function_args, session_id=session_id)
                    
                    yield {
                        "type": "tool_result",
                        "tool": function_name,
                        "result": result,
                        "status": "success"
                    }
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.get("id", "gemini_id"),
                        "name": function_name,
                        "content": result
                    })
                except Exception as e:
                    error_msg = f"Error ({function_name}): {str(e)}"
                    yield {
                        "type": "tool_result",
                        "tool": function_name,
                        "result": error_msg,
                        "status": "error"
                    }
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.get("id", "gemini_id"),
                        "name": function_name,
                        "content": error_msg
                    })
            
        except Exception as e:
            error_detail = f"Process Error: {str(e)}"
            yield {"type": "error", "error": error_detail}
            yield {"type": "done"}
            return
    
    yield {"type": "error", "error": "Max tool rounds reached."}
    yield {"type": "done"}